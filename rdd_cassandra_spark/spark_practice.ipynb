{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://dhcp-206-12-38-52.ubcvisitor.wireless.ubc.ca:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nc -l -p 9999 # to create a server on mac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### then connect jupyter with spark (in terminal) \n",
    "\n",
    "cd spark-2.3.0-bin-hadoop2.7\n",
    "\n",
    "export PYSPARK_DRIVER_PYTHON=jupyter\n",
    "\n",
    "export PYSPARK_DRIVER_PYTHON_OPTS='notebook'\n",
    "\n",
    "SPARK_LOCAL_IP=127.0.0.1 ./bin/pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as fn\n",
    "import pyspark.sql as sql\n",
    "\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"APP\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "sqlContext = SQLContext(spark)\n",
    "\n",
    "# Split the lines into words\n",
    "words = lines.select(\n",
    "   explode(\n",
    "       split(lines.value, \",\")\n",
    "   ).alias(\"word\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = words \\\n",
    "    .writeStream \\\n",
    "    .queryName(\"Query1\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|word|\n",
      "+----+\n",
      "| -20|\n",
      "| 100|\n",
      "|  40|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q1_3 = spark.sql(\"select * from Query1 order by word asc\")\n",
    "Q1_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+\n",
      "|minimum|maximum|average|\n",
      "+-------+-------+-------+\n",
      "|    -20|     40|   40.0|\n",
      "+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q1_4 = spark.sql(\"select min(word) as minimum, max(word) as maximum, avg(word) as average from Query1\")\n",
    "Q1_4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. type python server.py in terminal\n",
    "# 2. type nc localhost 9999 in another terminal # client connect on mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code check server.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Titanic\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "sqlContext = SQLContext(spark)\n",
    "\n",
    "titanic = lines.withColumn(\"PassengerId\", split(col(\"value\"), \",\").getItem(0).cast(IntegerType())).withColumn(\"Survived\", split(col(\"value\"), \",\").getItem(1).cast(IntegerType())).withColumn(\"Pclass\", split(col(\"value\"), \",\").getItem(2)).withColumn(\"Name\", split(col(\"value\"), \",\").getItem(3)).withColumn(\"Sex\", split(col(\"value\"), \",\").getItem(4)).withColumn(\"Age\", split(col(\"value\"), \",\").getItem(5).cast(DoubleType())).withColumn(\"Fare\", split(col(\"value\"), \",\").getItem(6).cast(DoubleType())).drop(\"value\")\n",
    "titanic.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = titanic \\\n",
    "    .writeStream \\\n",
    "    .queryName(\"Query2\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please wait for 10 seconds to run everything.\n",
    "query2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|   Fare|\n",
      "+-----------+--------+------+--------------------+------+----+-------+\n",
      "|          1|       0|     3| Mr. Owen Harris ...|  male|22.0|   7.25|\n",
      "|          5|       0|     3| Mr. William Henr...|  male|35.0|   8.05|\n",
      "|          9|       1|     3| Mrs. Oscar W (El...|female|27.0|11.1333|\n",
      "|         13|       0|     3| Mr. William Henr...|  male|20.0|   8.05|\n",
      "|         17|       0|     3| Master. Eugene Rice|  male| 2.0| 29.125|\n",
      "|         21|       0|     2| Mr. Joseph J Fynney|  male|35.0|   26.0|\n",
      "|         25|       0|     3| Miss. Torborg Da...|female| 8.0| 21.075|\n",
      "|         29|       1|     3|\" Miss. Ellen \"\"N...|female|null| 7.8792|\n",
      "|          2|       1|     1| Mrs. John Bradle...|female|38.0|71.2833|\n",
      "|          6|       0|     3|     Mr. James Moran|  male|null| 8.4583|\n",
      "+-----------+--------+------+--------------------+------+----+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q3 = spark.sql(\"select * from Query2\")\n",
    "Q3.show(10)\n",
    "#Q3.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. (1 mark) Write a SQL query to show the percentage of passengers who survived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       Percentage|\n",
      "+-----------------+\n",
      "|38.92694063926941|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select sum(Survived)/count(Survived) *100 as Percentage from Query2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Survived)|\n",
      "+-------------+\n",
      "|          341|\n",
      "+-------------+\n",
      "\n",
      "+---------------+\n",
      "|count(Survived)|\n",
      "+---------------+\n",
      "|            876|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select sum(Survived) from Query2\").show()\n",
    "spark.sql(\"select count(Survived) from Query2\").show()\n",
    "# therefore there is a bit difference of the final answer and the given answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Write a SQL query to show the number of passengers surviving under each gender category. When getting continuous data, did you see any pattern in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|   sex|Survived|\n",
      "+------+--------+\n",
      "|female|     233|\n",
      "|  male|     108|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select sex, sum(Survived) as Survived from Query2 group by sex\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obviously there are much more female survived than male. So, you may have more chance to survive as a female in this case (like a disaster)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. (1 mark) Write a SQL query to show the percentage of passengers who survived under each class category. Show the results in ascending order by `Pclass`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+\n",
      "|Pclass|         Percentage|\n",
      "+------+-------------------+\n",
      "|     1| 0.2408675799086758|\n",
      "|     2|0.20319634703196346|\n",
      "|     3| 0.5559360730593608|\n",
      "+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q3_5=spark.sql(\"select Pclass, count(Pclass)/(select count(*) from Query2) as Percentage from Query2 group by Pclass order by Pclass asc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please run serverQ4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"OMG\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "sqlContext = SQLContext(spark)\n",
    "\n",
    "# Split the lines into words\n",
    "words_TT = lines.select(\n",
    "   explode(\n",
    "       split(lines.value, \" \")\n",
    "   ).alias(\"TTword\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(words_TT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select words contain #\n",
    "words_TT = words_TT.where(words_TT.TTword.like(\"#%\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "query3 = words_TT \\\n",
    "    .writeStream \\\n",
    "    .queryName(\"Query3\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It takes a while to run everything. maybe 10 minute ish. Try drink a coffee and come back, it may still running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "query3.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|      TTword|Count|\n",
      "+------------+-----+\n",
      "|     #Hiring|37964|\n",
      "|       #Jobs|24776|\n",
      "|  #CareerArc|21845|\n",
      "|        #Job|21368|\n",
      "|        #job|17763|\n",
      "|     #Retail| 7867|\n",
      "|#Hospitality| 7664|\n",
      "|       #job?| 7569|\n",
      "|    #hiring!| 6860|\n",
      "|       #Job:| 5953|\n",
      "+------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select TTword, count(*) as Count from Query3 group by TTword order by Count desc\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you Matt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
